{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab60d19-91a2-4451-a546-b2ed28dd3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner \n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a85e66-5b6f-49b9-857c-2cb80d3ee3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "# PROJECT_ID = \"hzchen-lowa\"\n",
    "# MODEL_PATH='gs://'+PROJECT_ID+'-bucket/model/'\n",
    "# DATASET_PATH='gs://'+PROJECT_ID+'/area_cover_dataset.csv'\n",
    "# PIPELINE_ROOT = 'gs://'+PROJECT_ID\n",
    "# MODEL_ARTIFACTS_LOCATION ='gs://'+PROJECT_ID+'-bucket/'\n",
    "\n",
    "MODEL_PATH='gs://'+\"hzchen-lowa\"+'-bucket/model/'\n",
    "DATASET_PATH='gs://'+\"hzchen-lowa\"+'/area_cover_dataset.csv'\n",
    "PIPELINE_ROOT = 'gs://'+\"hzchen-lowa\"\n",
    "MODEL_ARTIFACTS_LOCATION ='gs://'+\"hzchen-lowa\"+'-bucket/'\n",
    "\n",
    "# MODEL_PATH='gs://hzchen-lowa/cepf005/staging/model/'\n",
    "# DATASET_PATH='gs://hzchen-lowa/cepf005/area_cover_dataset.csv'\n",
    "# PIPELINE_ROOT = 'gs://hzchen-lowa/cepf005/'\n",
    "# MODEL_ARTIFACTS_LOCATION ='gs://hzchen-lowa/cepf005/staging/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42396aa8-ca07-4aa2-9301-0ac8e8e700e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the area_cover_dataset csv data into pandas dataframe\n",
    "area_cover_dataframe = pandas.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07eb6c2-d7e1-4075-aff1-f2c6582a8bc7",
   "metadata": {},
   "source": [
    "**Task 4** Create the function that converts categorical data to indexed integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100ba233-6650-4b05-b832-d846609f86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes the area cover dataframe and converts the two categorical (string) columns into indexed values\n",
    "def index(dataframe):\n",
    "    \n",
    "    dataframe['Wilderness_Area'] = dataframe['Wilderness_Area'].astype('category').cat.codes\n",
    "    dataframe['Soil_Type'] = dataframe['Soil_Type'].astype('category').cat.codes\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307a3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_dataframe = index(area_cover_dataframe)\n",
    "features_dataframe = indexed_dataframe.drop(\"Area_Cover\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd67b4-142e-4772-9514-97c4ce735a0b",
   "metadata": {},
   "source": [
    "**Task 5** Extract the feature columns and standardize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f4df68-a401-4274-bead-d32ef844377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature columns into a new dataframe called scaler_features that has been standardized using the sklearn.preprocessing.StandardScaler method.\n",
    "# The features are all columns from the area cover dataset except the \"Area_Cover\" column\n",
    "indexed_dataframe = index(area_cover_dataframe)\n",
    "features_dataframe = indexed_dataframe.drop(\"Area_Cover\", axis = 1)\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "scaled_features = standard_scaler.fit_transform(features_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99cccc50-4302-4c5f-9deb-961993a64d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary matrix containing the categorical Area_Cover column data converted using keras.utils.to_categorical()\n",
    "labels_dataframe = indexed_dataframe[\"Area_Cover\"]\n",
    "categorical_labels = to_categorical(labels_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2e7306-3980-45c5-8cf9-e25879714261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into model training and validation data\n",
    "# dfx_train, dfx_val, dfy_train, dfy_val = train_test_split(scaled_features.values, categorical_labels, test_size=0.2)\n",
    "\n",
    "dfx_train, dfx_val, dfy_train, dfy_val = train_test_split(scaled_features, categorical_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e9fab-52e5-47cd-a30a-1ef367cdaff8",
   "metadata": {},
   "source": [
    "**Task 6** Create a function that returns a sequential categorical model function with a hyperparameter tuning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501648f6-defd-48c0-8ea4-e7133a85cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns a sequential categorical model function with a hyperparameter tuning layer\n",
    "def build_model(hptune):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape = (12,), activation = \"relu\"))\n",
    "    \n",
    "    model.add(\n",
    "        Dense(\n",
    "            # Define the hyperparameter.\n",
    "            units=hptune.Int(\"units\", min_value=2, max_value=12, step=1),\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(7, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e96334-1c5d-4a3e-8163-5dc7aa30025b",
   "metadata": {},
   "source": [
    "**Task 7** Create a Keras Hyperband Hyperparameter tuner with an accuracy objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a3d1d0-7b7a-4b92-8c85-e8e33ebcb721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hpo_logs/cepf005/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from hpo_logs/cepf005/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 00:48:37.894215: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-20 00:48:37.894295: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gcplow01): /proc/driver/nvidia/version does not exist\n",
      "2022-06-20 00:48:37.898038: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create a Keras Hyperband Hyperparameter tuner with an accuracy objective\n",
    "\n",
    "tuner = keras_tuner.Hyperband(build_model,\n",
    "                            objective='val_accuracy',\n",
    "                            max_epochs=20,\n",
    "                            factor=3,\n",
    "                            directory='hpo_logs',\n",
    "                            project_name='cepf005')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4f814-939a-45fd-a232-4bbf18f6e657",
   "metadata": {},
   "source": [
    "**Task 8** Perform Hyperparameter tuning and train the optimal model\n",
    "\n",
    "You do not have to add any of your own code for this task. Run the cells to tune, optimize and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d20fa022-34e6-4002-80fc-bd9891968da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Define an early stopping callback using that stops when the validation loss quantity does not improve after 5 epochs\n",
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Perform a Keras Tuner Search for the best hyperparameter configurations using the training data split over 50 epochs\n",
    "tuner.search(dfx_train, dfy_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters for the model as determined from the search\n",
    "best_hyperparameters=tuner.get_best_hyperparameters(num_trials=10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3872d0f-d519-4836-978f-4fd75caf5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.7388 - accuracy: 0.6973 - val_loss: 0.6447 - val_accuracy: 0.7286\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6272 - accuracy: 0.7346 - val_loss: 0.6059 - val_accuracy: 0.7408\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6038 - accuracy: 0.7420 - val_loss: 0.5888 - val_accuracy: 0.7506\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5872 - accuracy: 0.7492 - val_loss: 0.5775 - val_accuracy: 0.7530\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5731 - accuracy: 0.7537 - val_loss: 0.5597 - val_accuracy: 0.7600\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.5610 - accuracy: 0.7601 - val_loss: 0.5532 - val_accuracy: 0.7574\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.5511 - accuracy: 0.7628 - val_loss: 0.5468 - val_accuracy: 0.7669\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5426 - accuracy: 0.7671 - val_loss: 0.5461 - val_accuracy: 0.7680\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5343 - accuracy: 0.7706 - val_loss: 0.5275 - val_accuracy: 0.7744\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5258 - accuracy: 0.7765 - val_loss: 0.5237 - val_accuracy: 0.7742\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5191 - accuracy: 0.7780 - val_loss: 0.5268 - val_accuracy: 0.7699\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5128 - accuracy: 0.7803 - val_loss: 0.5174 - val_accuracy: 0.7814\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5079 - accuracy: 0.7835 - val_loss: 0.5146 - val_accuracy: 0.7793\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5028 - accuracy: 0.7857 - val_loss: 0.5080 - val_accuracy: 0.7816\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4992 - accuracy: 0.7878 - val_loss: 0.4991 - val_accuracy: 0.7884\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4935 - accuracy: 0.7925 - val_loss: 0.5018 - val_accuracy: 0.7844\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4906 - accuracy: 0.7913 - val_loss: 0.5058 - val_accuracy: 0.7814\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4878 - accuracy: 0.7929 - val_loss: 0.4988 - val_accuracy: 0.7831\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4848 - accuracy: 0.7937 - val_loss: 0.4979 - val_accuracy: 0.7856\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4825 - accuracy: 0.7935 - val_loss: 0.4906 - val_accuracy: 0.7895\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4788 - accuracy: 0.7957 - val_loss: 0.4922 - val_accuracy: 0.7853\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4757 - accuracy: 0.7968 - val_loss: 0.4860 - val_accuracy: 0.7909\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4744 - accuracy: 0.7981 - val_loss: 0.4825 - val_accuracy: 0.7952\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4709 - accuracy: 0.8001 - val_loss: 0.4836 - val_accuracy: 0.7926\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4689 - accuracy: 0.8008 - val_loss: 0.4781 - val_accuracy: 0.7954\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4654 - accuracy: 0.8017 - val_loss: 0.4859 - val_accuracy: 0.7915\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4657 - accuracy: 0.8016 - val_loss: 0.4797 - val_accuracy: 0.7951\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4637 - accuracy: 0.8021 - val_loss: 0.4919 - val_accuracy: 0.7913\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4611 - accuracy: 0.8046 - val_loss: 0.4919 - val_accuracy: 0.7895\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4595 - accuracy: 0.8046 - val_loss: 0.4841 - val_accuracy: 0.7889\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4578 - accuracy: 0.8052 - val_loss: 0.4836 - val_accuracy: 0.7925\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4564 - accuracy: 0.8057 - val_loss: 0.4728 - val_accuracy: 0.7993\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.4528 - accuracy: 0.8081 - val_loss: 0.4677 - val_accuracy: 0.8034\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4527 - accuracy: 0.8072 - val_loss: 0.4641 - val_accuracy: 0.8048\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4520 - accuracy: 0.8084 - val_loss: 0.4820 - val_accuracy: 0.7915\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4491 - accuracy: 0.8085 - val_loss: 0.4779 - val_accuracy: 0.7898\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4461 - accuracy: 0.8110 - val_loss: 0.4765 - val_accuracy: 0.7958\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4465 - accuracy: 0.8099 - val_loss: 0.4824 - val_accuracy: 0.7986\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4467 - accuracy: 0.8115 - val_loss: 0.4686 - val_accuracy: 0.8019\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4434 - accuracy: 0.8125 - val_loss: 0.4710 - val_accuracy: 0.7986\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4428 - accuracy: 0.8118 - val_loss: 0.4703 - val_accuracy: 0.8018\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4412 - accuracy: 0.8131 - val_loss: 0.4668 - val_accuracy: 0.8037\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4415 - accuracy: 0.8124 - val_loss: 0.4598 - val_accuracy: 0.8045\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4394 - accuracy: 0.8141 - val_loss: 0.4646 - val_accuracy: 0.8037\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4377 - accuracy: 0.8146 - val_loss: 0.4656 - val_accuracy: 0.8027\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4362 - accuracy: 0.8136 - val_loss: 0.4539 - val_accuracy: 0.8076\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4353 - accuracy: 0.8167 - val_loss: 0.4672 - val_accuracy: 0.8024\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.4343 - accuracy: 0.8159 - val_loss: 0.4549 - val_accuracy: 0.8069\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4343 - accuracy: 0.8155 - val_loss: 0.4588 - val_accuracy: 0.8055\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4337 - accuracy: 0.8164 - val_loss: 0.4576 - val_accuracy: 0.8063\n"
     ]
    }
   ],
   "source": [
    "# Create a new model using the best_hyperparameters and train it. \n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "history = model.fit(dfx_train, dfy_train, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e98cdbd-13ce-4abc-abf6-b95e8df87f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 46\n"
     ]
    }
   ],
   "source": [
    "# Using the model training history find and print out the epoch with the best validation accuracy. \n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e47b3543-7e34-4d89-a24c-4bed0fa0f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4514 - accuracy: 0.8124\n",
      "[Model test loss, test accuracy]: [0.45144736766815186, 0.8123999834060669]\n"
     ]
    }
   ],
   "source": [
    "# Print out the Model test loss and test accuracy by evaluating the validation data split. \n",
    "eval_result = model.evaluate(dfx_val, dfy_val)\n",
    "print(\"[Model test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068eb393-2586-44b8-a407-f3d5f5425bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/46\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.7302 - accuracy: 0.6989 - val_loss: 0.6396 - val_accuracy: 0.7293\n",
      "Epoch 2/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6204 - accuracy: 0.7365 - val_loss: 0.6061 - val_accuracy: 0.7431\n",
      "Epoch 3/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5999 - accuracy: 0.7445 - val_loss: 0.5873 - val_accuracy: 0.7474\n",
      "Epoch 4/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5846 - accuracy: 0.7497 - val_loss: 0.5752 - val_accuracy: 0.7561\n",
      "Epoch 5/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5714 - accuracy: 0.7555 - val_loss: 0.5681 - val_accuracy: 0.7564\n",
      "Epoch 6/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5591 - accuracy: 0.7606 - val_loss: 0.5517 - val_accuracy: 0.7663\n",
      "Epoch 7/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5495 - accuracy: 0.7634 - val_loss: 0.5402 - val_accuracy: 0.7726\n",
      "Epoch 8/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5407 - accuracy: 0.7679 - val_loss: 0.5402 - val_accuracy: 0.7721\n",
      "Epoch 9/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5322 - accuracy: 0.7710 - val_loss: 0.5231 - val_accuracy: 0.7758\n",
      "Epoch 10/46\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.5259 - accuracy: 0.7742 - val_loss: 0.5289 - val_accuracy: 0.7710\n",
      "Epoch 11/46\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5191 - accuracy: 0.7778 - val_loss: 0.5203 - val_accuracy: 0.7763\n",
      "Epoch 12/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5126 - accuracy: 0.7806 - val_loss: 0.5084 - val_accuracy: 0.7822\n",
      "Epoch 13/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5079 - accuracy: 0.7828 - val_loss: 0.5137 - val_accuracy: 0.7769\n",
      "Epoch 14/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5012 - accuracy: 0.7852 - val_loss: 0.5069 - val_accuracy: 0.7846\n",
      "Epoch 15/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4982 - accuracy: 0.7871 - val_loss: 0.5131 - val_accuracy: 0.7831\n",
      "Epoch 16/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4947 - accuracy: 0.7894 - val_loss: 0.5088 - val_accuracy: 0.7816\n",
      "Epoch 17/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4915 - accuracy: 0.7893 - val_loss: 0.4974 - val_accuracy: 0.7900\n",
      "Epoch 18/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4875 - accuracy: 0.7933 - val_loss: 0.4947 - val_accuracy: 0.7879\n",
      "Epoch 19/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4851 - accuracy: 0.7929 - val_loss: 0.4977 - val_accuracy: 0.7926\n",
      "Epoch 20/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4828 - accuracy: 0.7940 - val_loss: 0.5100 - val_accuracy: 0.7829\n",
      "Epoch 21/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4800 - accuracy: 0.7957 - val_loss: 0.4917 - val_accuracy: 0.7884\n",
      "Epoch 22/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4784 - accuracy: 0.7970 - val_loss: 0.4937 - val_accuracy: 0.7869\n",
      "Epoch 23/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4743 - accuracy: 0.7984 - val_loss: 0.4873 - val_accuracy: 0.7893\n",
      "Epoch 24/46\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.4718 - accuracy: 0.8003 - val_loss: 0.4994 - val_accuracy: 0.7891\n",
      "Epoch 25/46\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4720 - accuracy: 0.7993 - val_loss: 0.4900 - val_accuracy: 0.7916\n",
      "Epoch 26/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4690 - accuracy: 0.8001 - val_loss: 0.4812 - val_accuracy: 0.7981\n",
      "Epoch 27/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4660 - accuracy: 0.8030 - val_loss: 0.4814 - val_accuracy: 0.7942\n",
      "Epoch 28/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4649 - accuracy: 0.8040 - val_loss: 0.4772 - val_accuracy: 0.7976\n",
      "Epoch 29/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4637 - accuracy: 0.8040 - val_loss: 0.4809 - val_accuracy: 0.7953\n",
      "Epoch 30/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4601 - accuracy: 0.8051 - val_loss: 0.4827 - val_accuracy: 0.7957\n",
      "Epoch 31/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4594 - accuracy: 0.8058 - val_loss: 0.4768 - val_accuracy: 0.7994\n",
      "Epoch 32/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4597 - accuracy: 0.8058 - val_loss: 0.4857 - val_accuracy: 0.7949\n",
      "Epoch 33/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4572 - accuracy: 0.8069 - val_loss: 0.4788 - val_accuracy: 0.7964\n",
      "Epoch 34/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4551 - accuracy: 0.8066 - val_loss: 0.4826 - val_accuracy: 0.7938\n",
      "Epoch 35/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4541 - accuracy: 0.8086 - val_loss: 0.4821 - val_accuracy: 0.7934\n",
      "Epoch 36/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4530 - accuracy: 0.8080 - val_loss: 0.4728 - val_accuracy: 0.8014\n",
      "Epoch 37/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4513 - accuracy: 0.8076 - val_loss: 0.4721 - val_accuracy: 0.8024\n",
      "Epoch 38/46\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.4511 - accuracy: 0.8091 - val_loss: 0.4807 - val_accuracy: 0.7942\n",
      "Epoch 39/46\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4494 - accuracy: 0.8102 - val_loss: 0.4830 - val_accuracy: 0.7922\n",
      "Epoch 40/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4479 - accuracy: 0.8102 - val_loss: 0.4717 - val_accuracy: 0.7986\n",
      "Epoch 41/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4481 - accuracy: 0.8103 - val_loss: 0.4712 - val_accuracy: 0.8011\n",
      "Epoch 42/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4460 - accuracy: 0.8113 - val_loss: 0.4890 - val_accuracy: 0.7946\n",
      "Epoch 43/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4443 - accuracy: 0.8130 - val_loss: 0.4956 - val_accuracy: 0.7906\n",
      "Epoch 44/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4424 - accuracy: 0.8123 - val_loss: 0.4934 - val_accuracy: 0.7929\n",
      "Epoch 45/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4434 - accuracy: 0.8125 - val_loss: 0.4837 - val_accuracy: 0.7966\n",
      "Epoch 46/46\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4420 - accuracy: 0.8131 - val_loss: 0.4819 - val_accuracy: 0.7954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f270c43c890>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new model (hypermodel) using the best_hyperparameters and retrain. \n",
    "hypermodel = tuner.hypermodel.build(best_hyperparameters)\n",
    "# Retrain the model using the number of epochs that was previously determined to be the best. \n",
    "hypermodel.fit(dfx_train, dfy_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58b5c52-354a-4b7d-a114-c05864713f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4747 - accuracy: 0.8004\n",
      "[Hypermodel test loss, test accuracy]: [0.47471776604652405, 0.8004000186920166]\n"
     ]
    }
   ],
   "source": [
    "# Print out the test loss and test accuracy for hypermodel by evaluating the validation data split. \n",
    "eval_result = hypermodel.evaluate(dfx_val, dfy_val)\n",
    "print(\"[Hypermodel test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5165b086-9077-473c-b6eb-75bfb8fee14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 00:55:38.837569: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://hzchen-lowa-bucket/model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the hypertuned model\n",
    "# NB the MODEL_PATH bucket must be created before this will succeed and it must be in the same location as the model.\n",
    "# e.g. gsutil mb -l us-central1  gs://${PROJECT_ID}-bucket\n",
    "hypermodel.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48c72f-8554-4e5d-91c6-b3a3c76bf0ee",
   "metadata": {},
   "source": [
    "**Task 9** Create a Custom Container for Vertex AI pipeline model training\n",
    "1. Create a Python model trainer module using the above code\n",
    "2. Save the code as `model.py` in the `model/trainer` beneath the current working directory for this notebook\n",
    "3. Make sure you set the Project ID correctly in the Python script. \n",
    "4. Create the Dockerfile definition in the `model/` directory for your custom training container using the `gcr.io/deeplearning-platform-release/tf2-cpu.2-6` base container image\n",
    "\n",
    "Once you have prepared the custom container Python module code and Dockerfile you can build and test the custom container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752898e-d2d3-479a-b699-36b438a6cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the container using the following gcr.io tag\n",
    "IMAGE_URI=\"gcr.io/{}/tensorflow:latest\".format(PROJECT_ID)\n",
    "!docker build ./model/. -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b3e00-773e-48c1-bca0-d53b0cd60fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the docker image locally to test it\n",
    "!docker run $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aba246-f866-49d6-b1ab-1a752ede3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the docker image to the Google container registry\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226af430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kubeflow pipeline SDK and google cloud pipeline component for building Vertex AI pipelines\n",
    "!pip3 install kfp google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d386dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required for Vertext AI pipelines\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd71b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = MODEL_ARTIFACTS_LOCATION\n",
    "# project = PROJECT_ID\n",
    "# gcp_region = REGION\n",
    "# container_uri = \"gcr.io/dave-selfstudy01/tensorflow:latest\"\n",
    "\n",
    "# training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "#     display_name=\"tensorflow-train-model\",\n",
    "#     container_uri=container_uri,\n",
    "#     project=project,\n",
    "#     location=gcp_region,\n",
    "#     staging_bucket=bucket,\n",
    "#     training_fraction_split=0.8,\n",
    "#     validation_fraction_split=0.1,\n",
    "#     test_fraction_split=0.1,\n",
    "#     model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\",\n",
    "#     machine_type=\"n1-standard-4\"       \n",
    "# )\n",
    "\n",
    "# training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_op.outputs[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f1aed-476a-47d4-bc9d-93b6dea74a4a",
   "metadata": {},
   "source": [
    "**Task 10** Define the Vertex AI Training pipeline\n",
    "\n",
    "1. Add your code for the Training Operation using your newly created custom container\n",
    "    * This should reference the custom container_uri passed in as a parameter\n",
    "    * This should use \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\" for the `model_serving_container_image_uri`\n",
    "2. Add your code for the Model Deploy Operation\n",
    "    * This operation should output a model and an endpoint.\n",
    "    \n",
    "All machine types should be specified as \"n1-standard-4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7fbd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vertex AI pipeline\n",
    "@kfp.dsl.pipeline(name=\"vertex-ai-pipeline\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bucket: str = MODEL_ARTIFACTS_LOCATION,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    container_uri: str = \"\",\n",
    "):\n",
    "    \n",
    "    training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=\"tensorflow-train-model\",\n",
    "        container_uri=container_uri,\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        staging_bucket=bucket,\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1,\n",
    "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        base_output_dir=bucket,\n",
    "    )\n",
    "       \n",
    "    create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name = \"tensorflow-model-endpoint\",\n",
    "    )\n",
    "    \n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(        \n",
    "        endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        dedicated_resources_min_replica_count=2,\n",
    "        dedicated_resources_max_replica_count=10,\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        traffic_split={\"0\": 100}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1f5e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gcp/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Compile the  Vertex AI pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ffbb0-4b0d-40a9-bab2-85822d2d9638",
   "metadata": {},
   "source": [
    "**Task 11** Create the Vertex AI Pipeline job object\n",
    "\n",
    "The pipeline job must specified using the compiled pipeline definition JSON file and should point to your saved model location and your custom training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35996508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Vertex AI Pipeline job object\n",
    "pipeline_job = aiplatform.PipelineJob(  \n",
    "    display_name = \"cepf005_pipeline\",\n",
    "    template_path = \"pipeline.json\",\n",
    "    parameter_values = {\"container_uri\": \"gcr.io/dave-selfstudy01/tensorflow:latest\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d69f353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.auth\n",
    "\n",
    "# credentials, project_id = google.auth.default()\n",
    "# credentials, project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23de36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials.service_account_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca8cbf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vertex-ai-pipeline-20220620021533?project=886724937990\n",
      "PipelineJob projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/886724937990/locations/us-central1/pipelineJobs/vertex-ai-pipeline-20220620021533\n"
     ]
    }
   ],
   "source": [
    "# Run the Vertex AI pipeline job\n",
    "pipeline_job.run(service_account=\"dave-selfstudy-demo-783@dave-selfstudy01.iam.gserviceaccount.com\")\n",
    "# pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e69b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "MODEL_ID             DISPLAY_NAME\n",
      "4895962550765617152  tensorflow-train-model-model\n",
      "2000504232133787648  LEYI_VIP_20219175857\n",
      "3060538994426118144  SKAB-BQML-Kmeans01\n",
      "87037340454748160    SKAB-AutoML-import3\n",
      "8180005870839529472  SKAB-AutoML-import2-bad\n",
      "4879993243883798528  SKAB-AutoML-import-bad\n",
      "2826351813802852352  SKAB-clean-GS_202182954748\n"
     ]
    }
   ],
   "source": [
    "# List the model created by the pipeline\n",
    "!gcloud ai models list --region=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33838709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "ENDPOINT_ID          DISPLAY_NAME\n",
      "5757737774383890432  tensorflow-model-endpoint\n",
      "349433591400235008   SKAB-BQML-Kmeans-endpoint01\n",
      "5352932777408856064  SKAB-AutoML-endpoint\n",
      "Vertex AI Endpoint ID:5757737774383890432\n"
     ]
    }
   ],
   "source": [
    "# Store the endpoint ID where the model has been deployed \n",
    "\n",
    "!gcloud ai endpoints list --region=$REGION\n",
    "ENDPOINT_IDS=!gcloud ai endpoints list --region=$REGION --format=\"value(name)\" 2>/dev/null\n",
    "print(\"Vertex AI Endpoint ID:\" + ENDPOINT_IDS[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ca90da2-f6f2-48aa-aa05-8e1c2c24e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://sureskills-lab-dev/CEPF/vertex-ai/test.json...\n",
      "/ [1 files][ 22.0 KiB/ 22.0 KiB]                                                \n",
      "Operation completed over 1 objects/22.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Copy in the pre-prepared sample test.json \n",
    "!gsutil cp gs://sureskills-lab-dev/CEPF/vertex-ai/test.json . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44672e50-b94e-43a0-9816-f8523feda8a7",
   "metadata": {},
   "source": [
    "**Task 12** Create a function to convert the source JSON test data to an array of normalized column values\n",
    "\n",
    "The test data consists of samples with feature data that you want to use to generate area_cover type predictions using the model endpoint. \n",
    "\n",
    "You must define a functon that performs the following tasks:\n",
    "1. Read the `test.json` instance data into a dataframe\n",
    "2. Normalize the column data using the `StandardScalar.fit_transform` method\n",
    "3. Output an array of arrays containing the normalized feature column data for each test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "52ed3eee-c931-4ba0-bcc0-f2ab5daf8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the json test data to an array of standard scaler normalized column data\n",
    "\n",
    "def get_instances(file_name):\n",
    "    # instances = []\n",
    "\n",
    "    df = pandas.read_json('test.json')\n",
    "    df['Wilderness_Area'] = df.Wilderness_Area.astype('category').cat.codes\n",
    "    df['Soil_Type'] = df.Soil_Type.astype('category').cat.codes\n",
    "    standard_scaler = StandardScaler()\n",
    "    instances = standard_scaler.fit_transform(df).tolist()\n",
    "\n",
    "    # normalize_df = df\n",
    "\n",
    "    # for _ in normalize_df.values:\n",
    "    #     instances.append(list(_))\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b501bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = \"test.json\"\n",
    "# instances = get_instances(FILE_NAME)\n",
    "# instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "365fd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = hypermodel.predict(instances)\n",
    "# import numpy as np\n",
    "# np.argmax(preds, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b8291b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.protobuf import json_format\n",
    "# from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3f973588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = [json_format.ParseDict(instance_dict, Value()) for instance_dict in instances.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "70193c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7ca082f9-3ec6-44e1-af13-c73b24ca1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for making predictions using the endpoint\n",
    "def endpoint_predict(project: str, location: str, instances, endpoint: str):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "    endpoint = aiplatform.Endpoint(endpoint)   \n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c5bbdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the result by calling get_values() that convert JSON to the numpy array\n",
    "# Replace the endpoint ID with the new ENDPOINT_ID if needed\n",
    "FILE_NAME = \"test.json\"\n",
    "instances = get_instances(FILE_NAME)\n",
    "prediction_result = endpoint_predict(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    instances=instances,\n",
    "    endpoint=ENDPOINT_IDS[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "863bd27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.93716383, 0.0621097945, 1.37595324e-09, 1.99738046e-21, 0.000645427383, 2.18312968e-09, 8.10352576e-05], [0.0578953698, 0.942091, 1.51431504e-12, 6.53345959e-34, 1.2443269e-05, 1.24841421e-13, 1.18084336e-06], [0.759457111, 0.239930719, 2.94277047e-09, 7.90986776e-21, 0.000612155942, 2.54897294e-08, 7.65784325e-09], [0.457372069, 0.541426063, 1.50434101e-08, 3.81425304e-22, 0.000884396723, 2.42730702e-09, 0.000317476864], [0.37649858, 0.6003474, 8.25667448e-05, 3.89687761e-11, 0.0228658486, 0.000204873984, 6.49348749e-07], [0.0305828564, 0.967416763, 1.53155817e-08, 1.53276195e-23, 0.00189073232, 4.66670436e-09, 0.000109669934], [0.0786813796, 0.921168208, 5.83852289e-14, 1.86628517e-37, 3.45642366e-05, 1.26103733e-14, 0.000115819705], [0.178011775, 0.817611516, 8.14428436e-09, 7.71439859e-24, 0.00160957943, 1.20851651e-09, 0.00276722759], [0.269340038, 0.0002835372, 3.97127307e-32, 0.0, 3.54522381e-21, 1.08376845e-27, 0.730376363], [0.0746046901, 0.924553931, 4.28730756e-16, 0.0, 3.4766008e-06, 1.77028546e-17, 0.000837885251], [0.28274104, 0.716637135, 1.04830669e-07, 1.53890581e-19, 0.000375819131, 2.17924327e-08, 0.000245960779], [0.767998636, 2.5048781e-05, 5.16015904e-36, 0.0, 1.45029687e-28, 1.04995391e-30, 0.231976226], [0.641148865, 0.00255099, 2.75750202e-33, 0.0, 2.3470826e-26, 1.19690104e-30, 0.356300145], [0.945573807, 0.0532172434, 3.08369941e-10, 1.43065372e-19, 0.00120855693, 3.21993326e-07, 5.62421333e-12], [0.00105920504, 0.227902725, 0.14775902, 0.00306923315, 0.0314804688, 0.588729382, 1.51663127e-10], [0.958097696, 0.0250925012, 3.35455795e-20, 0.0, 2.543615e-13, 8.61548933e-19, 0.0168097671], [0.440523744, 0.529713631, 6.47633306e-06, 1.0002701e-15, 0.00904159155, 5.60539547e-07, 0.0207139105], [0.00534794666, 0.990340769, 2.71821355e-05, 4.79951399e-14, 0.00420311932, 8.10865822e-05, 2.11271015e-10], [0.793312073, 0.20423156, 1.47429596e-10, 7.98616e-25, 0.00232942449, 9.60914459e-10, 0.000126945946], [0.0524456874, 3.09427e-05, 1.0227601e-37, 0.0, 5.11990789e-34, 2.70245164e-34, 0.947523355], [0.0836354718, 0.915461481, 7.00814624e-08, 4.23706063e-21, 0.000900694577, 1.9666933e-08, 2.24962719e-06], [0.108162567, 0.0028478906, 7.16984288e-25, 0.0, 4.04269644e-26, 2.02102949e-24, 0.888989508], [0.00223940774, 0.992551565, 0.000237506523, 1.88000928e-13, 0.0049329903, 3.82926264e-05, 3.22451911e-07], [0.607294, 0.392657936, 2.06646286e-19, 0.0, 1.99214037e-06, 3.1326904e-19, 4.60551e-05], [0.384683251, 0.614205837, 1.35895544e-08, 4.26952171e-22, 0.0010568141, 4.43675097e-09, 5.40732581e-05], [0.513668776, 0.000276581, 3.9803246e-35, 0.0, 1.62413712e-23, 2.20645838e-30, 0.486054569], [0.00710894493, 0.246577471, 0.0610789135, 0.00323915528, 0.0487225205, 0.633272946, 1.45510076e-10], [0.938395798, 0.0614382587, 6.60432199e-14, 2.94818832e-32, 0.000104355175, 4.1326293e-13, 6.16367e-05], [0.727082729, 0.272905499, 7.40439949e-15, 2.54795387e-37, 5.29553608e-06, 2.20800737e-15, 6.51958089e-06], [0.667580366, 0.331967026, 1.0213913e-10, 1.34786808e-26, 0.000261567242, 5.20735226e-11, 0.00019102334], [0.863761604, 0.136237755, 7.54498899e-17, 0.0, 5.3086103e-07, 3.36298087e-17, 1.86074132e-07], [8.48259379e-11, 0.00327031803, 0.89424628, 0.000857054605, 3.64864936e-05, 0.101589821, 5.27317079e-16], [0.000539487752, 0.242374927, 0.627521574, 0.000834598206, 0.0432588756, 0.0854665786, 3.88269518e-06], [0.138825729, 0.859678268, 1.55808166e-07, 4.60397484e-20, 0.00148441282, 3.94436448e-08, 1.13858396e-05], [0.439460337, 0.557396352, 5.44686332e-14, 7.9552269e-34, 4.25891358e-08, 6.16807698e-15, 0.00314326282], [0.0208983161, 0.976406157, 3.18172465e-06, 2.01418831e-18, 0.00230306177, 1.24779518e-07, 0.000389235705], [0.643742263, 0.240724474, 2.37823837e-21, 0.0, 5.75464468e-11, 1.88119176e-20, 0.115533195], [0.000502653769, 0.348157048, 0.572177529, 9.84030557e-05, 0.0439915322, 0.035044387, 2.8544e-05], [0.00756383548, 0.900589526, 0.0327637196, 2.02668062e-07, 0.0527777895, 0.00629358832, 1.13018859e-05], [0.0623984486, 0.910608768, 0.000584196532, 1.66132275e-10, 0.0259369202, 0.000470474595, 1.16570402e-06], [0.000381027639, 0.977868736, 0.000267675874, 5.65069855e-13, 0.0209872965, 0.000495215587, 1.78848047e-08], [0.0740676075, 0.916212499, 3.76083844e-05, 1.64318876e-13, 0.00965091586, 3.09217721e-05, 5.45611954e-07], [0.0297562052, 0.905274272, 2.67869314e-06, 1.55110825e-14, 0.0645126253, 0.00045425765, 4.35501468e-10], [0.86583966, 0.130122, 9.12269286e-07, 4.28087226e-13, 0.00401305314, 2.43349896e-05, 1.61773295e-09], [0.511431158, 0.48814553, 5.09766491e-21, 0.0, 7.67917587e-12, 1.45818401e-21, 0.000423372083], [0.0307570398, 0.956298649, 0.00018423119, 3.50605569e-12, 0.0126438523, 0.000115835566, 4.44036516e-07], [1.87125916e-05, 0.999903202, 2.98942808e-07, 5.71433682e-25, 5.61138258e-05, 4.19369206e-10, 2.15518485e-05], [0.0108179823, 0.978756964, 0.000292692304, 3.9289739e-12, 0.00999039132, 0.000141769371, 1.77241375e-07], [0.977823138, 5.65446353e-05, 3.58759019e-31, 0.0, 6.98552715e-22, 5.64945252e-25, 0.0221203621], [0.764215469, 0.0249775667, 5.81393453e-23, 0.0, 4.38411945e-16, 3.66665255e-21, 0.210807011]], deployed_model_id='7387878111771099136', explanations=None)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6362ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0 \n",
      "1 : 1 \n",
      "2 : 0 \n",
      "3 : 1 \n",
      "4 : 1 \n",
      "5 : 1 \n",
      "6 : 1 \n",
      "7 : 1 \n",
      "8 : 6 \n",
      "9 : 1 \n",
      "10 : 1 \n",
      "11 : 0 \n",
      "12 : 0 \n",
      "13 : 0 \n",
      "14 : 5 \n",
      "15 : 0 \n",
      "16 : 1 \n",
      "17 : 1 \n",
      "18 : 0 \n",
      "19 : 6 \n",
      "20 : 1 \n",
      "21 : 6 \n",
      "22 : 1 \n",
      "23 : 0 \n",
      "24 : 1 \n",
      "25 : 0 \n",
      "26 : 5 \n",
      "27 : 0 \n",
      "28 : 0 \n",
      "29 : 0 \n",
      "30 : 0 \n",
      "31 : 2 \n",
      "32 : 2 \n",
      "33 : 1 \n",
      "34 : 1 \n",
      "35 : 1 \n",
      "36 : 0 \n",
      "37 : 2 \n",
      "38 : 1 \n",
      "39 : 1 \n",
      "40 : 1 \n",
      "41 : 1 \n",
      "42 : 1 \n",
      "43 : 0 \n",
      "44 : 0 \n",
      "45 : 1 \n",
      "46 : 1 \n",
      "47 : 1 \n",
      "48 : 0 \n",
      "49 : 0 \n"
     ]
    }
   ],
   "source": [
    "# Save `Area_Cover` predictions with respect to the test instance features\n",
    "area_cover_predictions={}\n",
    "for index,area_cover in enumerate(prediction_result.predictions):\n",
    "    print(index,\":\",numpy.argmax(area_cover), end=' \\n')\n",
    "    area_cover_predictions[index]=str(numpy.argmax(area_cover))\n",
    "    \n",
    "f = open(\"predictions.txt\", \"w\")\n",
    "f.write(json.dumps(area_cover_predictions))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5c30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('gcp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb375cd48549538ae6419718afb30fb24337720f3a48e69a0312c0663df6cdfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
